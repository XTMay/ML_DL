# 机器学习核心算法详解

## 1. 非线性算法 (Nonlinear Algorithms)

### 理论基础

**线性 vs 非线性**：
- **线性算法**：假设输入特征与输出之间存在线性关系，如 y = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
- **非线性算法**：能够捕捉特征间复杂的非线性关系，处理更复杂的数据模式

### 核心概念

**为什么需要非线性？**
现实世界中的大多数问题都是非线性的：
- 图像识别：像素间的复杂空间关系
- 自然语言：词汇间的语义关系
- 金融预测：市场因素间的复杂交互

### 主要非线性算法类型

#### 1. 核方法 (Kernel Methods)
- **SVM with RBF kernel**：将数据映射到高维空间
- **核函数**：K(x, x') = exp(-γ||x - x'||²)
- **优势**：在原始空间计算，避免高维计算复杂性

#### 2. 神经网络
```
激活函数引入非线性：
- ReLU: f(x) = max(0, x)
- Sigmoid: f(x) = 1/(1 + e^(-x))
- Tanh: f(x) = (e^x - e^(-x))/(e^x + e^(-x))
```

#### 3. 决策树类算法
- 通过递归分割创建非线性决策边界
- 能够处理特征间的复杂交互

### 实际应用场景
- **计算机视觉**：物体识别、人脸检测
- **自然语言处理**：机器翻译、情感分析
- **推荐系统**：用户行为建模

---

## 2. 主成分分析 (Principal Component Analysis, PCA)

### 核心理论

**目标**：在保留最大信息量的前提下降低数据维度

**数学基础**：
- 寻找数据变化最大的方向（主成分）
- 主成分是协方差矩阵的特征向量
- 按特征值大小排序，选择前k个主成分

### 算法步骤

#### 标准PCA流程：
1. **数据标准化**：X_centered = X - mean(X)
2. **计算协方差矩阵**：C = (1/n) * X_centered^T * X_centered
3. **特征值分解**：C = V * Λ * V^T
4. **选择主成分**：选择前k个最大特征值对应的特征向量
5. **数据投影**：Y = X_centered * V_k

### 几何直观

**二维例子**：
```
原始数据呈椭圆分布：
- 第一主成分：椭圆长轴方向（最大方差）
- 第二主成分：椭圆短轴方向（次大方差，与第一主成分正交）
```

### 方差解释

**累积方差贡献率**：
- 第k个主成分解释的方差比例 = λₖ / Σλᵢ
- 前k个主成分的累积贡献率 = (Σᵢ₌₁ᵏλᵢ) / (Σᵢ₌₁ⁿλᵢ)
- 通常选择累积贡献率达到85%-95%的主成分数量

### 优缺点分析

**优点**：
- 有效降维，减少计算复杂度
- 去除特征间的相关性
- 噪声过滤效果
- 可用于数据可视化

**缺点**：
- 线性变换，无法处理非线性关系
- 主成分的物理意义不明确
- 对异常值敏感
- 需要标准化预处理

### 应用场景
- **图像压缩**：保留主要视觉信息
- **基因数据分析**：识别主要遗传模式
- **金融风险建模**：提取主要风险因子
- **推荐系统**：用户-物品矩阵降维

---

## 3. 朴素贝叶斯 (Naive Bayes)

### 理论基础

**贝叶斯定理**：
```
P(类别|特征) = P(特征|类别) * P(类别) / P(特征)

数学表达式：
P(Cₖ|x₁,x₂,...,xₙ) = P(x₁,x₂,...,xₙ|Cₖ) * P(Cₖ) / P(x₁,x₂,...,xₙ)
```

**"朴素"假设**：
特征之间条件独立，即：
P(x₁,x₂,...,xₙ|Cₖ) = P(x₁|Cₖ) * P(x₂|Cₖ) * ... * P(xₙ|Cₖ)

### 核心算法

#### 训练阶段：
1. **计算先验概率**：P(Cₖ) = 类别k的样本数 / 总样本数
2. **计算似然概率**：P(xᵢ|Cₖ) 对每个特征在每个类别下的分布

#### 预测阶段：
1. **计算后验概率**：对每个类别计算 P(Cₖ|x)
2. **选择最大概率类别**：ŷ = argmax P(Cₖ|x)

### 三种主要变体

#### 1. 高斯朴素贝叶斯
- **适用**：连续特征
- **假设**：特征服从正态分布
- **概率密度**：P(xᵢ|Cₖ) = (1/√(2πσₖᵢ²)) * exp(-(xᵢ-μₖᵢ)²/(2σₖᵢ²))

#### 2. 多项式朴素贝叶斯
- **适用**：离散特征（如文本词频）
- **概率计算**：P(xᵢ|Cₖ) = (Nₖᵢ + α) / (Nₖ + α*n)
- **平滑参数**：α防止零概率问题

#### 3. 伯努利朴素贝叶斯
- **适用**：二值特征（0/1）
- **概率计算**：P(xᵢ=1|Cₖ) = pₖᵢ，P(xᵢ=0|Cₖ) = 1-pₖᵢ

### 优缺点分析

**优点**：
- 算法简单，训练和预测速度快
- 对小样本数据效果好
- 对缺失数据不敏感
- 不需要迭代优化
- 理论基础扎实

**缺点**：
- 特征独立假设过于强烈，现实中很少成立
- 对特征间相关性处理不好
- 需要平滑技术处理零概率
- 连续特征需要分布假设

### 应用场景
- **垃圾邮件过滤**：基于词频特征分类
- **文本情感分析**：正面/负面情感判断
- **医疗诊断**：基于症状进行疾病预测
- **新闻分类**：自动分类新闻文章

---

## 4. K-最近邻算法 (K-Nearest Neighbors, KNN)

### 核心思想

**基本假设**：相似的样本应该有相似的标签
**算法原理**：对于新样本，找到训练集中最相似的k个样本，基于这k个邻居进行预测

### 算法流程

#### 分类任务：
1. **计算距离**：新样本与所有训练样本的距离
2. **找到k个最近邻**：按距离排序，取前k个
3. **多数投票**：k个邻居中出现最多的类别作为预测结果

#### 回归任务：
1. **找到k个最近邻**：同分类
2. **平均预测**：ŷ = (1/k) * Σyᵢ（k个邻居标签的平均值）

### 距离度量方式

#### 1. 欧几里得距离
```
d(x, y) = √(Σᵢ(xᵢ - yᵢ)²)
最常用，适合连续特征
```

#### 2. 曼哈顿距离
```
d(x, y) = Σᵢ|xᵢ - yᵢ|
对异常值较不敏感
```

#### 3. 闵可夫斯基距离
```
d(x, y) = (Σᵢ|xᵢ - yᵢ|ᵖ)^(1/p)
p=1时为曼哈顿距离，p=2时为欧几里得距离
```

#### 4. 汉明距离
```
适用于离散/二值特征
计算不同位置的个数
```

### 关键参数选择

#### k值选择：
- **k=1**：对噪声敏感，容易过拟合
- **k过大**：边界过于平滑，可能欠拟合
- **经验法则**：k = √n，其中n为训练样本数
- **交叉验证**：系统性选择最优k值

#### 权重方案：
- **等权重**：所有邻居权重相同
- **距离加权**：wᵢ = 1/dᵢ，距离越近权重越大

### 算法优化

#### 1. 数据预处理
- **特征标准化**：防止大数值特征主导距离计算
- **特征选择**：去除不相关特征，降低"维度诅咒"

#### 2. 高效搜索结构
- **KD-Tree**：适用于低维数据（d<20）
- **Ball-Tree**：适用于高维数据
- **LSH（局部敏感哈希）**：近似最近邻搜索

### 优缺点分析

**优点**：
- 算法简单直观，易于理解和实现
- 无需假设数据分布
- 适用于多分类问题
- 可以处理非线性问题
- 对局部模式敏感

**缺点**：
- 计算复杂度高（O(nd)），预测慢
- 存储需求大，需保存全部训练数据
- 对不相关特征敏感
- 在高维空间性能下降（维度诅咒）
- 对不平衡数据敏感

### 应用场景
- **推荐系统**：基于用户相似性推荐
- **图像识别**：基于像素相似性分类
- **异常检测**：识别与正常样本差异大的点
- **模式识别**：手写数字识别

---

## 5. 分类回归树 (Classification And Regression Trees, CART)

### 核心理论

**决策树思想**：通过一系列if-else规则对数据进行递归分割，构建树形决策结构

**CART特点**：
- 二叉树结构（每个内部节点只有两个分支）
- 既可用于分类也可用于回归
- 使用代理分割处理缺失值

### 分割准则

#### 分类任务 - 基尼不纯度
```
基尼不纯度：Gini(D) = 1 - Σᵢ pᵢ²

其中 pᵢ 是类别i在数据集D中的比例

信息增益：ΔGini = Gini(D) - Σⱼ (|Dⱼ|/|D|) * Gini(Dⱼ)
```

**基尼不纯度直觉**：
- Gini = 0：数据完全纯净（只有一个类别）
- Gini最大：数据类别分布最均匀

#### 回归任务 - 均方误差
```
MSE(D) = (1/|D|) * Σᵢ (yᵢ - ȳ)²

其中 ȳ 是数据集D中目标值的均值

分割后的MSE减少：ΔMSE = MSE(D) - Σⱼ (|Dⱼ|/|D|) * MSE(Dⱼ)
```

### 构建算法

#### 递归分割过程：
1. **选择最佳分割**：
   - 遍历所有特征和可能的分割点
   - 计算每个分割的不纯度减少
   - 选择不纯度减少最大的分割

2. **分割条件**：
   - 数值特征：x ≤ threshold
   - 类别特征：x ∈ subset

3. **停止条件**：
   - 达到最大深度
   - 节点样本数少于最小分割样本数
   - 不纯度减少小于阈值
   - 叶子节点样本数少于最小叶子样本数

4. **叶子节点预测**：
   - 分类：多数类别
   - 回归：平均值

### 剪枝技术

#### 1. 预剪枝（Pre-pruning）
- **在构建过程中**提前停止分割
- **参数控制**：
  - max_depth：最大深度
  - min_samples_split：最小分割样本数
  - min_samples_leaf：最小叶子样本数
  - min_impurity_decrease：最小不纯度减少

#### 2. 后剪枝（Post-pruning）
- **先构建完整树**，再删除不重要的分支
- **代价复杂性剪枝**：
  ```
  成本函数：C(T) = Σₗ Nₗ * Rₗ + α * |T|
  
  其中：
  - Nₗ：叶子节点l的样本数
  - Rₗ：叶子节点l的错误率
  - |T|：叶子节点数
  - α：复杂性参数
  ```

### 处理缺失值

#### CART的代理分割方法：
1. **构建代理变量列表**：找到与主分割变量最相关的其他变量
2. **分割时使用代理**：如果主变量缺失，使用代理变量进行分割
3. **预测时处理缺失**：按照训练时的策略使用代理分割

### 优缺点分析

**优点**：
- **可解释性强**：决策路径清晰直观
- **处理混合数据**：同时处理数值和类别特征
- **无需特征预处理**：对特征缩放不敏感
- **自动特征选择**：选择最有用的特征进行分割
- **处理非线性关系**：通过递归分割捕捉复杂模式
- **处理缺失值**：内置缺失值处理机制

**缺点**：
- **容易过拟合**：特别是深度较大时
- **不稳定性**：数据小变化可能导致树结构大变化
- **偏向多值特征**：倾向于选择取值较多的特征
- **难以处理线性关系**：对简单线性关系效率不高
- **预测能力有限**：单棵树的预测能力有限

### 应用场景

#### 1. 医疗诊断
```
根节点：体温 > 37.5°C?
├─ 是：白细胞计数 > 10000?
│  ├─ 是：细菌感染（概率0.85）
│  └─ 否：病毒感染（概率0.70）
└─ 否：健康（概率0.95）
```

#### 2. 金融风险评估
```
根节点：年收入 > 50000?
├─ 是：信用历史良好?
│  ├─ 是：低风险
│  └─ 否：中风险
└─ 否：高风险
```

#### 3. 客户细分
- 基于购买行为、人口统计特征进行客户分类
- 为不同客户群体制定营销策略

### 扩展算法

#### 1. 随机森林
- 多棵CART树的集成
- Bootstrap采样 + 随机特征选择
- 提高稳定性和预测精度

#### 2. 梯度提升树
- 顺序构建多棵树
- 每棵树拟合前一棵树的残差
- 强大的预测能力

---

## 总结对比

| 算法 | 类型 | 优势 | 劣势 | 适用场景 |
|------|------|------|------|----------|
| 非线性算法 | 多种 | 处理复杂关系 | 计算复杂，易过拟合 | 复杂模式识别 |
| PCA | 无监督降维 | 降维去噪，计算高效 | 线性变换，解释性差 | 数据预处理，可视化 |
| Naive Bayes | 分类 | 简单快速，小样本友好 | 独立假设强 | 文本分类，垃圾邮件 |
| KNN | 分类/回归 | 简单直观，无参数假设 | 计算慢，存储大 | 推荐系统，模式识别 |
| CART | 分类/回归 | 可解释性强，处理混合数据 | 易过拟合，不稳定 | 医疗诊断，风险评估 |

这些算法各有特色，在实际应用中常常结合使用，形成更强大的机器学习解决方案。