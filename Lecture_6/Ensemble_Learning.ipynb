{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XTMay/ML_DL/blob/main/Lecture_6/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d5bf260",
      "metadata": {
        "id": "5d5bf260"
      },
      "source": [
        "# 集成学习在发票信息提取中的应用教学 (Ensemble Learning)\n",
        "## 概述\n",
        "集成学习通过组合多个模型来提高预测性能，在发票信息提取任务中可以显著提升准确率和鲁棒性。本教学将详细介绍Bagging、Boosting和Stacking三种方法在发票OCR中的应用"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b78cdb9",
      "metadata": {
        "id": "9b78cdb9"
      },
      "source": [
        "## 1. Bagging（Bootstrap Aggregating 自助聚集）\n",
        "### 理论基础\n",
        "Bagging通过自助采样创建多个训练子集，训练多个模型并平均预测结果，减少方差"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c157727d",
      "metadata": {
        "id": "c157727d"
      },
      "source": [
        "\t•\t原理：从原始训练集通过有放回抽样（Bootstrap）生成多个子训练集；分别训练多个独立（通常是决策树）模型，再对它们输出进行平均（回归）或投票（分类）汇总 ￼。\n",
        "\t•\t好处：极大减少模型的方差，抑制过拟合，提升鲁棒性；典型应用如随机森林。\n",
        "\t•\t缺点：仅对减小方差有效，对偏差（bias）改进有限，如果基础模型关联性太高，效果有限。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1671547e",
      "metadata": {
        "id": "1671547e"
      },
      "source": [
        "## How Does Bagging Work?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fb7f54b",
      "metadata": {
        "id": "0fb7f54b"
      },
      "source": [
        "![How Does Bagging Work?](https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/03/bagging-1024x576.webp?resize=1024%2C576&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f763175",
      "metadata": {
        "id": "1f763175"
      },
      "source": [
        "### ensemble_bagging_invoice.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b9e60d2",
      "metadata": {
        "id": "6b9e60d2"
      },
      "source": [
        "## 2. Boosting（自适应提升） XGBoost?\n",
        "### 理论基础\n",
        "Boosting通过序列化训练弱学习器，每个新模型专注于前一个模型的错误，逐步提升性能。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a94de3a2",
      "metadata": {
        "id": "a94de3a2"
      },
      "source": [
        "### ensemble_boosting_invoice.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ad0bea",
      "metadata": {
        "id": "32ad0bea"
      },
      "source": [
        "\t•\t原理：顺序训练一系列弱学习器，每个新模型关注纠正前一个模型的错误。常见算法 AdaBoost（自适应提升）或 Gradient Boosting（梯度提升）。\n",
        "\t•\t好处：显著减少偏差，提高模型准确性；对弱模型进行增强。\n",
        "\t•\t缺点：容易过拟合，尤其当迭代太多时；对噪声敏感，计算复杂度和时间也较大。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a59805c8",
      "metadata": {
        "id": "a59805c8"
      },
      "source": [
        "## How Does Boosting Work?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d629c67a",
      "metadata": {
        "id": "d629c67a"
      },
      "source": [
        "![How Does Boosting Work?](https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/03/boosting-1024x576.webp?resize=1024%2C576&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f85a4e",
      "metadata": {
        "id": "03f85a4e"
      },
      "source": [
        "## 3. Stacking（堆叠泛化）\n",
        "### 理论基础\n",
        "Stacking使用元学习器来学习如何最优地组合基础模型的预测结果。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62722c1c",
      "metadata": {
        "id": "62722c1c"
      },
      "source": [
        "### ensemble_stacking_invoice.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb730f1",
      "metadata": {
        "id": "4cb730f1"
      },
      "source": [
        "\t•\t原理：并行训练多种不同类型的基础模型（如决策树、支持向量机等），然后将它们的预测作为特征输入给“元模型”（meta-model），由元模型学习最佳组合方式输出最终预测 ￼。\n",
        "\t•\t好处：能有效整合多种模型优势，进一步提高表现；适合结构多样的基学习器。\n",
        "\t•\t缺点：实现更复杂；训练过程耗时，计算资源要求高；解释性变差。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e08362a",
      "metadata": {
        "id": "3e08362a"
      },
      "source": [
        "## How does Stacking Work?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "694f9b3a",
      "metadata": {
        "id": "694f9b3a"
      },
      "source": [
        "![How does Stacking Work?](https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/03/stacking-1024x576.webp?resize=1024%2C576&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2caaacdf",
      "metadata": {
        "id": "2caaacdf"
      },
      "source": [
        "#### 集成学习的整体优势与注意\n",
        "\n",
        "> 优势\n",
        "\t- 综合多个模型提升性能、减少错误（bias + variance）。\n",
        "\t- 在大多数任务中显著优于单一模型 ￼。\n",
        "- 注意事项\n",
        "\t- 组合的模型必须具有多样性，才能获得协同效应()。\n",
        "\t- Boosting 的连锁误差与噪声敏感性需要谨慎控制。\n",
        "\t- Stacking 的多层结构使得调试、解释与部署更具挑战性。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}